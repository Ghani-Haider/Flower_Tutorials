{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cuda using PyTorch 2.2.1+cu121 and Flower 1.7.0\n"
     ]
    }
   ],
   "source": [
    "from os import cpu_count\n",
    "\n",
    "from collections import OrderedDict\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchinfo import summary\n",
    "\n",
    "import flwr as fl\n",
    "from flwr.common import Metrics\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\n",
    "    f\"Training on {DEVICE} using PyTorch {torch.__version__} and Flower {fl.__version__}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "\n",
    "NUM_CLIENT = 10\n",
    "EPOCHS_CLIENT = 1\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKER = cpu_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Load And Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading The Dataset And Partitioning ###\n",
    "\n",
    "def load_datasets(num_clients: int):\n",
    "    \n",
    "    # image transformation\n",
    "    img_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "    \n",
    "    # loading the torchvision dataset\n",
    "    train_dataset = CIFAR10(\n",
    "        root='./dataset',\n",
    "        train=True,\n",
    "        transform=img_transform,\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    test_dataset = CIFAR10(\n",
    "        root='./dataset',\n",
    "        train=False,\n",
    "        transform=img_transform,\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    # split training dataset into partitions\n",
    "    partition_size = len(train_dataset) // num_clients\n",
    "    lengths = [partition_size] * num_clients\n",
    "    train_part_dataset = random_split(\n",
    "        dataset=train_dataset,\n",
    "        lengths=lengths,\n",
    "        generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "    \n",
    "    # create all client train and val dataloaders\n",
    "    train_dataloaders = []\n",
    "    val_dataloaders = []\n",
    "\n",
    "    # split partition into train and val datasets and wrap into torch dataloaders\n",
    "    for dataset in train_part_dataset:\n",
    "        # split the partition\n",
    "        split_dataset = random_split(\n",
    "            dataset=dataset,\n",
    "            lengths=[0.8, 0.2], # train & val dataset split fraction\n",
    "            generator=torch.Generator().manual_seed(42)\n",
    "        )\n",
    "        # wrap with torch dataloader and add to dataloader list\n",
    "        partition_train_dl = DataLoader(\n",
    "            dataset=split_dataset[0],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=NUM_WORKER\n",
    "        )\n",
    "        partition_val_dl = DataLoader(\n",
    "            dataset=split_dataset[1],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_workers=NUM_WORKER\n",
    "        )\n",
    "        train_dataloaders.append(partition_train_dl)\n",
    "        val_dataloaders.append(partition_val_dl)\n",
    "        \n",
    "    # create test dataloader from the test split (Dataset) with transform function\n",
    "    test_dataloader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKER\n",
    "    )\n",
    "\n",
    "    # return all the train (partitioned), val (partitioned) & test dataloaders\n",
    "    return train_dataloaders, val_dataloaders, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./dataset/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:23<00:00, 7369375.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/cifar-10-python.tar.gz to ./dataset\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataloaders, val_dataloaders, test_dataloader = load_datasets(NUM_CLIENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture, Train & Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining The Model ###\n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"Creates the TinyVGG architecture for 32*32 Image Data\"\"\"\n",
    "        \n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=10, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features= 10 * 8 * 8, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        return self.classifier(self.block2(self.block1(x)))\n",
    "\n",
    "### Train and Test Function For FLOWER Clients\n",
    "    \n",
    "# calculate accuracy\n",
    "def accuracy_fn(y_pred: torch.tensor, y_true: torch.tensor) -> float:\n",
    "    \"\"\"Calculates the accuracy of a model on given predictions\n",
    "\n",
    "    Args:\n",
    "        y_pred: predicted labels\n",
    "        y_true: true labels\n",
    "    \n",
    "    Returns:\n",
    "        A float value which is the calculated accuracy.\n",
    "    \"\"\"\n",
    "    return ((torch.eq(y_pred, y_true).sum().item() / len(y_true)) * 100)\n",
    "\n",
    "# fit the model on training data\n",
    "def train(model: torch.nn.Module,\n",
    "          data_loader: torch.utils.data.DataLoader,\n",
    "          epochs: int,\n",
    "          device: torch.device,\n",
    "          lr: float,\n",
    "          verbose=False,\n",
    "          loss_fn: torch.nn.Module = None,\n",
    "          optimizer: torch.optim.Optimizer = None) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a PyTorch model for the given epochs.\n",
    "\n",
    "    Turns a target PyTorch model to training mode and then\n",
    "    runs through all of the required training steps (forward\n",
    "    pass, loss calculation, optimizer step).\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to be trained.\n",
    "        dataloader: A DataLoader instance for the model to be trained on.\n",
    "        epochs: Epochs.\n",
    "        device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "        verbose: A boolean value to see the model metrics (loss and accuracy)\n",
    "        loss_fn: A PyTorch loss function to minimize.\n",
    "        optimizer: A PyTorch optimizer to help minimize the loss function.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of training loss and training accuracy metrics.\n",
    "        In the form (train_loss, train_accuracy). For example:\n",
    "\n",
    "        (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "\n",
    "    # optimizer and criterion (loss_fn) if None given\n",
    "    if loss_fn == None:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train() # model in train mode\n",
    "    total_epoch_loss, total_epoch_acc = 0, 0\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        train_loss, train_acc = 0, 0\n",
    "        \n",
    "        for X, y in data_loader:\n",
    "            # get data to device\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            y_logit = model(X)\n",
    "            loss = loss_fn(y_logit, y)        \n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad() # empty param's grad\n",
    "            loss.backward() # backward propagation\n",
    "            optimizer.step() # updata params (take the gradient descent step)\n",
    "\n",
    "            # Metrics\n",
    "            # calculate loss and accuracy per batch\n",
    "            train_loss += loss.item() * len(y)\n",
    "            y_pred_labels = torch.argmax(y_logit, dim=1)\n",
    "            train_acc += accuracy_fn(y_pred_labels, y)\n",
    "        \n",
    "        # per epoch\n",
    "        train_loss /= len(data_loader.dataset)\n",
    "        train_acc /= len(data_loader)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1} | Train Loss {train_loss:.4f} | Train Acc {train_acc:.2f}\")\n",
    "\n",
    "        # for all epochs\n",
    "        total_epoch_loss += train_loss\n",
    "        total_epoch_acc += train_acc\n",
    "    \n",
    "    return (total_epoch_loss / epochs, total_epoch_acc / epochs)\n",
    "    \n",
    "\n",
    "# test the model on test data\n",
    "def test(model: torch.nn.Module,\n",
    "         data_loader: torch.utils.data.DataLoader,\n",
    "         device: torch.device,\n",
    "         loss_fn: torch.nn.Module=None) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a PyTorch model for the given epochs.\n",
    "\n",
    "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
    "    a forward pass on a testing dataset.\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to be tested.\n",
    "        data_loader: A DataLoader instance for the model to be tested on.\n",
    "        device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
    "        loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
    "\n",
    "    Returns:\n",
    "        A tuple of testing loss and testing accuracy metrics.\n",
    "        In the form (test_loss, test_accuracy). For example:\n",
    "\n",
    "        (0.0223, 0.8985)\n",
    "    \"\"\"\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # criterion (loss_fn) if None given\n",
    "    if loss_fn == None:\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval() # model in evaluation mode\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            # get data to device\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # forward pss\n",
    "            y_logit = model(X)\n",
    "            loss = loss_fn(y_logit, y)\n",
    "\n",
    "            # calculate loss and accuracy per batch\n",
    "            test_loss += loss.item() * len(y)\n",
    "            y_pred_labels = torch.argmax(y_logit, dim=1)\n",
    "            test_acc += accuracy_fn(y_pred_labels, y)\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    test_acc /= len(data_loader)\n",
    "    return (test_loss, test_acc)\n",
    "\n",
    "### Updating Model Parameters (helper functions from client's perspective)\n",
    "\n",
    "# de-serialized & set client parameters\n",
    "def set_parameters(model: nn.Module, parameters: List[np.ndarray]):\n",
    "    # de-serialize the ndarray to tensors\n",
    "    parameters = [torch.from_numpy(np_arr).to(dtype=torch.float32, device=DEVICE) for np_arr in parameters]\n",
    "    # match every weight with its model block\n",
    "    param_dict = zip(model.state_dict().keys(), parameters)\n",
    "    # convert the param_dict to ordered dict and load back into the model\n",
    "    model.load_state_dict(OrderedDict(param_dict), strict=True)\n",
    "\n",
    "# get serialized parameters from client\n",
    "def get_parameters(model: nn.Module) -> List[np.ndarray]:\n",
    "    # serialize the model weights into ndarray and return\n",
    "    return [weights.cpu().numpy() for _, weights in model.state_dict().items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom FLOWER Client (sub-class of FLOWER NumPyClient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementing A FLOWER Client using NumPyClient\n",
    "\n",
    "class CustomClient(fl.client.NumPyClient):\n",
    "    \"\"\"A custom client implementation representing an organization with model and data\"\"\"\n",
    "\n",
    "    # pass a model, train and test dataloader\n",
    "    def __init__(self, client_id: int, model, train_dataloader, val_dataloader) -> None:\n",
    "        super().__init__()\n",
    "        self.cid = client_id\n",
    "        self.model = model\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "\n",
    "    # return the current local model parameters to server\n",
    "    def get_parameters(self, config):\n",
    "        print(f'[Client {self.cid}] get_parameters')\n",
    "        return get_parameters(self.model)\n",
    "    \n",
    "    # receive global model parameters, train and return the updated parameters with other metrics/arbitrary values\n",
    "    def fit(self, parameters, config):\n",
    "        print(f'[Client {self.cid}] fit, config: {config}')\n",
    "\n",
    "        # get learning rate from server\n",
    "        client_lr = config['lr']\n",
    "\n",
    "        set_parameters(self.model, parameters)\n",
    "        loss, accuracy = train(model=self.model, data_loader=self.train_dataloader, epochs=EPOCHS_CLIENT, device=DEVICE, lr=client_lr)\n",
    "        return get_parameters(self.model), len(self.train_dataloader), {'loss': loss, 'accuracy': accuracy} # updated parameters, num_batches, metrics\n",
    "    \n",
    "    # receive global model parameters, evaluate and return the metrics/arbitrary values\n",
    "    def evaluate(self, parameters, config) -> Tuple[float | int | Dict[str, bool | bytes | float | int | str]]:\n",
    "        print(f'[Client {self.cid}] evaluate, config: {config}')\n",
    "        set_parameters(self.model, parameters)\n",
    "        loss, accuracy = test(model=self.model, data_loader=self.val_dataloader, device=DEVICE)\n",
    "        return float(loss), len(self.val_dataloader), {'accuracy': accuracy} # loss, num_batches, metrics\n",
    "    \n",
    "### client_fun: To create a CustomClient instance on demand when requested from FLOWER Framework\n",
    "\n",
    "def client_fn(client_id: str) -> CustomClient:\n",
    "    \"\"\"Create a FLOWER client representing a single organization\n",
    "\n",
    "    Creates an instance of CustomClient based on the client_id provided\n",
    "    to load client specific data partition.\n",
    "\n",
    "    Args:\n",
    "        client_id: An str to load a specific client.\n",
    "\n",
    "    Returns:\n",
    "        A CustomClient (fl.client.NumPyClient) instance representing a FLOWER client.\n",
    "    \"\"\"\n",
    "\n",
    "    # create an instance of the model\n",
    "    model = TinyVGG().to(DEVICE)\n",
    "\n",
    "    # load the client specific dataloaders\n",
    "    train_dl = train_dataloaders[int(client_id)]\n",
    "    val_dl = val_dataloaders[int(client_id)]\n",
    "\n",
    "    # create client instance, convert object to Client type and return\n",
    "    return CustomClient(client_id=int(client_id), model=model, train_dataloader=train_dl, val_dataloader=val_dl).to_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions For Metric Aggregation, Central Evaluation & Configuring Client Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### client metric aggregation functions\n",
    "\n",
    "# aggregate the metrics received from all the client's evaluate function\n",
    "def eval_weighted_avg(metrics: List[Tuple[int, Metrics]]):\n",
    "    #  multiply accuracy with each client's number of samples / or is it batch size??\n",
    "    accuracies = [num_examples * m['accuracy'] for num_examples, m in metrics]\n",
    "    num_samples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    # aggregate and return the custom metrics (weighted avg)\n",
    "    return {'test_accuracy': sum(accuracies) / sum(num_samples)}\n",
    "\n",
    "# aggregate the metrics received from all the client's fit function\n",
    "def fit_weighted_avg(metrics: List[Tuple[int, Metrics]]):\n",
    "    #  multiply accuracy with each client's number of samples / or is it batch size??\n",
    "    accuracies = [num_examples * m['accuracy'] for num_examples, m in metrics]\n",
    "    num_samples = [num_examples for num_examples, _ in metrics]\n",
    "    # for loss\n",
    "    losses = [num_examples * m['loss'] for num_examples, m in metrics]\n",
    "\n",
    "    # aggregate and return the custom metrics (weighted avg)\n",
    "    return {'train_loss': sum(losses) / sum(num_samples), 'train_accuracy': sum(accuracies) / sum(num_samples)}\n",
    "\n",
    "\n",
    "### server-side parameter evaluation function\n",
    "# evaluate on the server\n",
    "def eval_server(server_round: int, params: fl.common.NDArray, config: Dict[str, fl.common.Scalar]):\n",
    "    # create server model\n",
    "    model = TinyVGG().to(DEVICE)\n",
    "    # load validation dataloader\n",
    "    val_dl = val_dataloaders[0]\n",
    "    # update model with latest parameters\n",
    "    set_parameters(model=model, parameters=params)\n",
    "    # perform centralized evaluation\n",
    "    loss, accuracy = test(model=model, data_loader=val_dl, device=DEVICE)\n",
    "    print(f'Server round {server_round}: evaluation loss {loss} | accuracy {accuracy}')\n",
    "    return loss, {'accuracy': accuracy}\n",
    "\n",
    "### sending/receiving arbitrary values to/from clients\n",
    "# configure/set client-side params from server side\n",
    "# training configuration from server to client\n",
    "def fit_config(server_round: int):\n",
    "    \"\"\"Return training configuration dict form server to client for each round.\n",
    "\n",
    "    Perform rounds of training with one local epoch.\n",
    "    \"\"\"\n",
    "    config = {\n",
    "        'server_round': server_round,\n",
    "        'local_epochs': 1\n",
    "    }\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building A Strategy From Scratch\n",
    "\n",
    "Overwrite the `configure_fit` method such that it passes a higher learning rate, while the rest will be kept the same as *FedAvg*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, Tuple, Union\n",
    "\n",
    "from flwr.common import (\n",
    "    EvaluateIns,\n",
    "    EvaluateRes,\n",
    "    FitIns,\n",
    "    FitRes,\n",
    "    MetricsAggregationFn,\n",
    "    NDArrays,\n",
    "    Parameters,\n",
    "    Scalar,\n",
    "    ndarrays_to_parameters,\n",
    "    parameters_to_ndarrays,\n",
    ")\n",
    "from flwr.server.client_manager import ClientManager\n",
    "from flwr.server.client_proxy import ClientProxy\n",
    "from flwr.server.strategy.aggregate import aggregate, weighted_loss_avg\n",
    "\n",
    "### custom strategy class\n",
    "class FedCustom(fl.server.strategy.Strategy):\n",
    "    def __init__(\n",
    "            self,\n",
    "            fraction_fit: float = 1.0,\n",
    "            fraction_evaluate: float = 1.0,\n",
    "            min_fit_clients: int = 2,\n",
    "            min_evaluate_clients: int = 2,\n",
    "            min_available_clients: int = 2,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.fraction_fit = fraction_fit\n",
    "        self.fraction_evaluate = fraction_evaluate\n",
    "        self.min_fit_clients = min_fit_clients\n",
    "        self.min_evaluate_clients = min_evaluate_clients\n",
    "        self.min_available_clients = min_available_clients\n",
    "\n",
    "    # returns a printable representation of the object\n",
    "    def __repr__(self) -> str:\n",
    "        return 'FedCustom'\n",
    "    \n",
    "    # Get available clients for training\n",
    "    def num_fit_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return number of clients for training & required number of clients.\"\"\"\n",
    "\n",
    "        num_clients = int(num_available_clients * self.fraction_fit)\n",
    "        return max(num_clients, self.min_fit_clients), self.min_available_clients\n",
    "    \n",
    "    # Get available clients for evaluation\n",
    "    def num_evaluation_clients(self, num_available_clients: int) -> Tuple[int, int]:\n",
    "        \"\"\"Return number of clients for evaluation & required number of clients.\"\"\"\n",
    "\n",
    "        num_clients = int(num_available_clients * self.fraction_evaluate)\n",
    "        return max(num_clients, self.min_evaluate_clients), self.min_available_clients\n",
    "\n",
    "    # initializes the parameters of the global model\n",
    "    def initialize_parameters(\n",
    "            self,\n",
    "            client_manager: ClientManager\n",
    "            ) -> Parameters | None:\n",
    "        \"\"\"Initialize global model parameters\"\"\"\n",
    "        model = TinyVGG()\n",
    "        ndarrays = get_parameters(model)\n",
    "        return fl.common.ndarrays_to_parameters(ndarrays)\n",
    "    \n",
    "    ### CUSTOMIZING THIS FUNCTION\n",
    "    ## remember, client lr is also implemented in train() & CustomClient class\n",
    "    # configure the next round of training\n",
    "    def configure_fit(\n",
    "            self,\n",
    "            server_round: int,\n",
    "            parameters: Parameters,\n",
    "            client_manager: ClientManager\n",
    "            ) -> List[Tuple[ClientProxy, FitIns]]:\n",
    "        \"\"\"Configure the next round of training\"\"\"\n",
    "\n",
    "        # sample clients\n",
    "        sample_size, min_num_clients = self.num_fit_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size,\n",
    "            min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # create custom configs\n",
    "        n_clients = len(clients)\n",
    "        half_clients = n_clients // 2\n",
    "        standard_config = {'lr': 0.001}\n",
    "        higher_lr_config = {'lr': 0.003}\n",
    "        fit_configurations = []\n",
    "        # for half clients set learning_rate as 0.001, for other half set it as 0.003\n",
    "        for idx, client in enumerate(clients):\n",
    "            if idx < half_clients:\n",
    "                fit_configurations.append((client, FitIns(parameters, standard_config)))\n",
    "            else:\n",
    "                fit_configurations.append((client, FitIns(parameters, higher_lr_config)))\n",
    "\n",
    "        return fit_configurations\n",
    "    \n",
    "    # aggregate the client updated parameters & metrics using weighted average (same as FedAvg)\n",
    "    def aggregate_fit(\n",
    "            self,\n",
    "            server_round: int,\n",
    "            results: List[Tuple[ClientProxy | FitRes]],\n",
    "            failures: List[Tuple[ClientProxy | FitRes] | BaseException]\n",
    "            ) -> Tuple[Parameters | None | Dict[str, bool | bytes | float | int | str]]:\n",
    "        \"\"\"Aggregate the fit results using weighted average\"\"\"\n",
    "\n",
    "        # get the updated parameter & client dataset size\n",
    "        weights_results = [(parameters_to_ndarrays(fit_res.parameters), fit_res.num_examples) for _, fit_res in results]\n",
    "        # aggregate the parameters\n",
    "        parameters_aggregated = ndarrays_to_parameters(aggregate(weights_results))\n",
    "        metrics_aggregated = {}\n",
    "\n",
    "        return parameters_aggregated, metrics_aggregated\n",
    "    \n",
    "    # setup the next round of evaluation by choosing clients & evaluate instructions\n",
    "    def configure_evaluate(\n",
    "            self,\n",
    "            server_round: int,\n",
    "            parameters: Parameters,\n",
    "            client_manager: ClientManager\n",
    "            ) -> List[Tuple[ClientProxy | EvaluateIns]]:\n",
    "        \"\"\"Configure the next round of evaluation\"\"\"\n",
    "\n",
    "        # no client evaluation\n",
    "        if self.fraction_evaluate == 0.0:\n",
    "            return []\n",
    "        \n",
    "        config = {}\n",
    "        evaluate_ins = EvaluateIns(parameters, config)\n",
    "\n",
    "        # sample clients\n",
    "        sample_size, min_num_clients = self.num_evaluation_clients(\n",
    "            client_manager.num_available()\n",
    "        )\n",
    "        clients = client_manager.sample(\n",
    "            num_clients=sample_size,\n",
    "            min_num_clients=min_num_clients\n",
    "        )\n",
    "\n",
    "        # return clients & evaluation configs\n",
    "        return [(client, evaluate_ins) for client in clients]\n",
    "    \n",
    "    # Aggregates evaluation results/metrics obtained from multiple clients\n",
    "    def aggregate_evaluate(\n",
    "            self,\n",
    "            server_round: int,\n",
    "            results: List[Tuple[ClientProxy | EvaluateRes]],\n",
    "            failures: List[Tuple[ClientProxy | EvaluateRes] | BaseException]\n",
    "            ) -> Tuple[float | None | Dict[str, bool | bytes | float | int | str]]:\n",
    "        \"\"\"Aggregates evaluation losses using weighted average.\"\"\"\n",
    "\n",
    "        if not results:\n",
    "            return None, {}\n",
    "        \n",
    "        loss_aggregated = weighted_loss_avg(\n",
    "            [(eval_res.num_examples, eval_res.loss) for _, eval_res in results]\n",
    "        )\n",
    "\n",
    "        metrics_aggregated = {}\n",
    "        return loss_aggregated, metrics_aggregated\n",
    "    \n",
    "    # Server-side evaluation of global model parameter\n",
    "    def evaluate(\n",
    "            self, \n",
    "            server_round: int, \n",
    "            parameters: Parameters\n",
    "            ) -> Tuple[float | Dict[str, bool | bytes | float | int | str]] | None:\n",
    "        \"\"\"Evaluate global model parameters using an evaluation function\"\"\"\n",
    "\n",
    "        # Won't perform the global model evaluation on the server side.\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO flwr 2024-03-25 14:34:32,042 | app.py:178 | Starting Flower simulation, config: ServerConfig(num_rounds=3, round_timeout=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 14:34:33,701\tINFO worker.py:1621 -- Started a local Ray instance.\n",
      "INFO flwr 2024-03-25 14:34:34,530 | app.py:213 | Flower VCE: Ray initialized with resources: {'CPU': 16.0, 'object_store_memory': 7130750976.0, 'node:__internal_head__': 1.0, 'memory': 14261501952.0, 'node:10.255.93.233': 1.0, 'accelerator_type:G': 1.0, 'GPU': 1.0}\n",
      "INFO flwr 2024-03-25 14:34:34,531 | app.py:219 | Optimize your simulation with Flower VCE: https://flower.dev/docs/framework/how-to-run-simulations.html\n",
      "INFO flwr 2024-03-25 14:34:34,531 | app.py:242 | Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 1}\n",
      "INFO flwr 2024-03-25 14:34:34,538 | app.py:288 | Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
      "INFO flwr 2024-03-25 14:34:34,539 | server.py:89 | Initializing global parameters\n",
      "INFO flwr 2024-03-25 14:34:34,541 | server.py:272 | Using initial parameters provided by strategy\n",
      "INFO flwr 2024-03-25 14:34:34,541 | server.py:91 | Evaluating initial parameters\n",
      "INFO flwr 2024-03-25 14:34:34,542 | server.py:104 | FL starting\n",
      "DEBUG flwr 2024-03-25 14:34:34,542 | server.py:222 | fit_round 1: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 0] fit, config: {'lr': 0.001}\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 1] fit, config: {'lr': 0.003}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-25 14:34:38,529 | server.py:236 | fit_round 1 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-25 14:34:38,531 | server.py:173 | evaluate_round 1: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-25 14:34:40,630 | server.py:187 | evaluate_round 1 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-25 14:34:40,631 | server.py:222 | fit_round 2: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 0] fit, config: {'lr': 0.001}\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 1] fit, config: {'lr': 0.003}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-25 14:34:43,278 | server.py:236 | fit_round 2 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-25 14:34:43,280 | server.py:173 | evaluate_round 2: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-25 14:34:45,317 | server.py:187 | evaluate_round 2 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-25 14:34:45,317 | server.py:222 | fit_round 3: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 0] fit, config: {'lr': 0.001}\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 1] fit, config: {'lr': 0.003}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-25 14:34:48,061 | server.py:236 | fit_round 3 received 2 results and 0 failures\n",
      "DEBUG flwr 2024-03-25 14:34:48,063 | server.py:173 | evaluate_round 3: strategy sampled 2 clients (out of 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 0] evaluate, config: {}\n",
      "\u001b[2m\u001b[36m(DefaultActor pid=740357)\u001b[0m [Client 1] evaluate, config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flwr 2024-03-25 14:34:50,085 | server.py:187 | evaluate_round 3 received 2 results and 0 failures\n",
      "INFO flwr 2024-03-25 14:34:50,085 | server.py:153 | FL finished in 15.543169245996978\n",
      "INFO flwr 2024-03-25 14:34:50,086 | app.py:226 | app_fit: losses_distributed [(1, 2.304007205963135), (2, 2.303844101905823), (3, 2.3037052812576295)]\n",
      "INFO flwr 2024-03-25 14:34:50,086 | app.py:227 | app_fit: metrics_distributed_fit {}\n",
      "INFO flwr 2024-03-25 14:34:50,086 | app.py:228 | app_fit: metrics_distributed {}\n",
      "INFO flwr 2024-03-25 14:34:50,086 | app.py:229 | app_fit: losses_centralized []\n",
      "INFO flwr 2024-03-25 14:34:50,087 | app.py:230 | app_fit: metrics_centralized {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "History (loss, distributed):\n",
       "\tround 1: 2.304007205963135\n",
       "\tround 2: 2.303844101905823\n",
       "\tround 3: 2.3037052812576295"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Start The Training\n",
    "\n",
    "# client resources (allocate cpus and gpus)\n",
    "client_resources = {'num_cpus': NUM_WORKER//NUM_CLIENT, 'num_gpus': 1 if torch.cuda.is_available() else 0}\n",
    "\n",
    "# start simulation\n",
    "fl.simulation.start_simulation(\n",
    "    client_fn=client_fn,\n",
    "    num_clients=2,\n",
    "    config=fl.server.ServerConfig(num_rounds=3),\n",
    "    strategy=FedCustom(), # <-- custom strategy\n",
    "    client_resources=client_resources\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
